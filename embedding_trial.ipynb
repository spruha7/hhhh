{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88268f1-5f89-4d4e-ad34-354dec6f632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import Crew, Agent, LLM, Task\n",
    "from crewai_tools import BaseTool\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.agents import Tool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bfcb75-87c6-4c5b-9146-2b20e58e2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_4o = LLM(\n",
    "    model=\"azure/gpt-4o\",\n",
    "    base_url=\"https://genai-openai-ai-mazinghacktivists.openai.azure.com/\",\n",
    "    api_key=\"446cd4ef4aad49e097e94e63a0593be2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2f0dce-282f-43c8-b6a3-5104c32bf8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Website Data Ingestion\n",
    "loader = UnstructuredURLLoader(urls=[\"https://docs.crewai.com/how-to/Installing-CrewAI/\"])\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb6ddc7-6f81-4398-b164-70dbd129444d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f112807b2a974be5b01c172f650edb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f85ebe768b4f6c817c16301f3cd70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfce128a5f514610b53d5e7ada7b09fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2035e13f4ba4309b4c5b9749e9831c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935df9a33be4491b8c9e7a52ec54b859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe57243818243cda67af0d090caa3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499724a107b444a89fe3a9bc47731d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0c66a269ca4dfabed8e3fd01512f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1f6e0de21548059efb06ef78bdc1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c00a8d207b04b908aa9457d3457f9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da11114ff335472cadf34ee9d7a1012f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Embeddings\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=model_name)\n",
    "\n",
    "# Create & Persist Vector Database\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"./chroma_db\")\n",
    "db.persist()\n",
    "\n",
    "# Define Retriever from Vector Store\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26557f3-02a6-4f3f-b91d-de2c6bb2d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBRetriever(BaseTool):\n",
    "    name: str = \"db_retriever\"\n",
    "    description: str = \"To search for documents related to the userâ€™s query.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        # Implementation goes here\n",
    "        return retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74a4b37-a9a5-4c5e-a23c-a7f4571e06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rwa_optimization_agent = Agent(\n",
    "    role=\"RWA Optimization Advisor\",\n",
    "    goal=\"Offers targeted recommendations to reduce RWA and suggest optimized capital allocation.\",\n",
    "    backstory=\"\"\"You are a senior RWA Optimization Advisor at Barclays Bank PLC, skilled in providing RWA Optimization \n",
    "    strategies. Your job  is to provide strategies for minimizing RWA, recommend actions such as asset reclassification, securitization ,\n",
    "    or reducing exposure high-risk assets. Besides, your job is to suggest optimized capital allocation to minimize RWA while managing risk exposure\n",
    "    \"\"\",\n",
    "    llm=llm_4o\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894c3588-d7fc-4b7d-8825-0d0b553fe21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rwa_optimization_task = Task(\n",
    "    description=\"Analyze the user input ({user_input} and identify key risks and RWA optimization opportunities\",\n",
    "    expected_output=\"A bullet list summary of top 5 most important RWA Optimization opportunities\",\n",
    "    agent = rwa_optimization_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b658632-17a8-4756-aeae-0dbeb5495fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRWA Optimization Advisor\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze the user input (Barclays wants to trade in India real estate sector with exposure of INR 1 billion. Please suggest RWA optimization strategy adhering to Basel 3 regulations. and identify key risks and RWA optimization opportunities\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRWA Optimization Advisor\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "1. **Asset Reclassification:**\n",
      "   - Reclassify certain real estate exposures to categories that attract lower risk weights under Basel 3 regulations. For example, shifting assets to properties with higher liquidity or to sectors with historically lower default rates can help reduce RWA.\n",
      "   \n",
      "2. **Securitization:**\n",
      "   - Securitize a portion of the real estate portfolio to transfer the credit risk to third-party investors. This can be achieved by creating asset-backed securities (ABS) or mortgage-backed securities (MBS), thereby reducing the bank's RWAs while maintaining exposure to the real estate sector.\n",
      "   \n",
      "3. **Geographic Diversification:**\n",
      "   - Diversify the real estate investments across different geographic locations within India to mitigate region-specific risks. This can help lower the overall risk profile of the portfolio and, consequently, the RWA.\n",
      "   \n",
      "4. **Engage in Public-Private Partnerships (PPPs):**\n",
      "   - Participate in PPP projects that often benefit from government support and guarantees, potentially leading to lower risk weights. These projects can support infrastructural development while presenting lower credit risk.\n",
      "   \n",
      "5. **Optimized Capital Allocation:**\n",
      "   - Allocate capital to real estate assets with lower risk weights and better credit profiles. Prioritize investments in commercial real estate with strong credit ratings and established tenants over highly speculative or undeveloped properties.\n",
      "   \n",
      "By implementing these strategies, Barclays can optimize its capital allocation, minimize RWA, and manage its risk exposure effectively while complying with Basel 3 regulations.\u001b[00m\n",
      "\n",
      "\n",
      "1. **Asset Reclassification:**\n",
      "   - Reclassify certain real estate exposures to categories that attract lower risk weights under Basel 3 regulations. For example, shifting assets to properties with higher liquidity or to sectors with historically lower default rates can help reduce RWA.\n",
      "   \n",
      "2. **Securitization:**\n",
      "   - Securitize a portion of the real estate portfolio to transfer the credit risk to third-party investors. This can be achieved by creating asset-backed securities (ABS) or mortgage-backed securities (MBS), thereby reducing the bank's RWAs while maintaining exposure to the real estate sector.\n",
      "   \n",
      "3. **Geographic Diversification:**\n",
      "   - Diversify the real estate investments across different geographic locations within India to mitigate region-specific risks. This can help lower the overall risk profile of the portfolio and, consequently, the RWA.\n",
      "   \n",
      "4. **Engage in Public-Private Partnerships (PPPs):**\n",
      "   - Participate in PPP projects that often benefit from government support and guarantees, potentially leading to lower risk weights. These projects can support infrastructural development while presenting lower credit risk.\n",
      "   \n",
      "5. **Optimized Capital Allocation:**\n",
      "   - Allocate capital to real estate assets with lower risk weights and better credit profiles. Prioritize investments in commercial real estate with strong credit ratings and established tenants over highly speculative or undeveloped properties.\n",
      "   \n",
      "By implementing these strategies, Barclays can optimize its capital allocation, minimize RWA, and manage its risk exposure effectively while complying with Basel 3 regulations.\n"
     ]
    }
   ],
   "source": [
    "# Create Crew\n",
    "research_crew1 = Crew(\n",
    "    agents=[rwa_optimization_agent],\n",
    "    tasks=[rwa_optimization_task],\n",
    "    verbose=True  # This will print logs to the console as the crew works\n",
    ")\n",
    "\n",
    "# Job Context\n",
    "job_crew_works = {\n",
    "    'user_input': 'Barclays wants to trade in India real estate sector with exposure of INR 1 billion. Please suggest RWA optimization strategy adhering to Basel 3 regulations.'\n",
    "}\n",
    "\n",
    "# Kickoff the Crew's Work\n",
    "result = research_crew1.kickoff(inputs=job_crew_works)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6fcd908-61eb-4c0a-bd78-207be0bca9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore, IndexManagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88384067-511c-4a1c-b5e3-c8c546064c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-textanalytics\n",
      "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl.metadata (82 kB)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from azure-ai-textanalytics) (1.32.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from azure-ai-textanalytics) (1.1.28)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from azure-ai-textanalytics) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from azure-ai-textanalytics) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2024.8.30)\n",
      "Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
      "Installing collected packages: azure-ai-textanalytics\n",
      "Successfully installed azure-ai-textanalytics-5.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-textanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f660f0de-c8de-4fbc-9a69-b33024a8d662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement azure-ai-openai (from versions: none)\n",
      "ERROR: No matching distribution found for azure-ai-openai\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f5933c6-7888-4637-86fe-9aca3fde2e0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure.ai.openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 19\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize Azure OpenAI and embedding models\u001b[39;00m\n\u001b[0;32m     11\u001b[0m llm \u001b[38;5;241m=\u001b[39m AzureOpenAI(\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39mAZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n\u001b[0;32m     13\u001b[0m     deployment_name\u001b[38;5;241m=\u001b[39mAZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     api_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-02-01\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIClient\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcredentials\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureKeyCredential\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'azure.ai.openai'"
     ]
    }
   ],
   "source": [
    "# Environment Variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://genai-openai-ai-mazinghacktivists.openai.azure.com/\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", \"66243e4ca95e4b46ae6b8a7cc29c7517\")\n",
    "AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME\", \"gpt-35-turbo\") # I'm using GPT-4o\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME\", \"text-embedding-3-large\") # I'm using text-embedding-3-large\n",
    "SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\", \"https://rwa.search.windows.net\")\n",
    "SEARCH_SERVICE_API_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\", \"nKmwEveDrRLrUCZHbzBO9rEQOByUS6Nv2lFAglTFiSAzSeChq7aX\")\n",
    "INDEX_NAME = \"json-vector\"\n",
    "\n",
    "# Initialize Azure OpenAI and embedding models\n",
    "llm = AzureOpenAI(\n",
    "    model=AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "    deployment_name=AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "from azure.ai.openai import OpenAIClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import numpy as np\n",
    "\n",
    "# Set up the Azure OpenAI embedding configuration\n",
    "class AzureOpenAIEmbedding:\n",
    "    def __init__(self, model, deployment_name, api_key, azure_endpoint, api_version=\"2024-02-01\"):\n",
    "        # Initialize the client with the given Azure credentials\n",
    "        self.client = AzureOpenAIClient(\n",
    "            endpoint=azure_endpoint,\n",
    "            credential=AzureKeyCredential(api_key)\n",
    "        )\n",
    "        self.model = model\n",
    "        self.deployment_name = deployment_name\n",
    "        self.api_version = api_version\n",
    "\n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        # Generate the embedding for the input text\n",
    "        response = self.client.embeddings(\n",
    "            model=self.model,\n",
    "            input=[text],  # List of texts to embed (here, we're embedding one text)\n",
    "            deployment_name=self.deployment_name,\n",
    "            api_version=self.api_version\n",
    "        )\n",
    "        \n",
    "        # Extract the embedding from the response\n",
    "        embedding = response[0]['embedding']\n",
    "        return np.array(embedding)\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "    deployment_name=AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "# Initialize search clients\n",
    "credential = AzureKeyCredential(SEARCH_SERVICE_API_KEY)\n",
    "index_client = SearchIndexClient(endpoint=SEARCH_SERVICE_ENDPOINT, credential=credential)\n",
    "search_client = SearchClient(endpoint=SEARCH_SERVICE_ENDPOINT, index_name=INDEX_NAME, credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e120423e-abf9-4d55-8964-da22a94eb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.azureaisearch import (\n",
    "    IndexManagement,\n",
    "    MetadataIndexFieldType,\n",
    ")\n",
    "import nest_asyncio\n",
    "from llama_index.core.extractors import TitleExtractor, QuestionsAnsweredExtractor\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bceb7573-be6c-40fb-92d6-3fa2002c5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_store = AzureAISearchVectorStore(\n",
    "        search_or_index_client=index_client,\n",
    "        index_name=INDEX_NAME,\n",
    "        index_management=IndexManagement.VALIDATE_INDEX,\n",
    "        id_field_key=\"parent_id\",\n",
    "        chunk_field_key=\"chunk_id\",\n",
    "        embedding_field_key=\"text_vector\",\n",
    "        embedding_dimensionality=3072,\n",
    "        doc_id_field_key=\"parent_id\",\n",
    "        language_analyzer=\"en.lucene\",\n",
    "        vector_algorithm_type=\"exhaustiveKnn\",\n",
    "        metadata_string_field_key=\"title\"\n",
    "    )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "        [],\n",
    "        storage_context=storage_context,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46054d72-e553-401e-8b65-4220a7d9cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",  # You can choose different models for embeddings\n",
    "        input=text\n",
    "    )\n",
    "    # Extract the embedding vector from the response\n",
    "    embedding = response['data'][0]['embedding']\n",
    "    return np.array(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14cfbec7-f9d4-4144-8fa5-8b4ca7f86db7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AzureOpenAIEmbedding' object has no attribute 'get_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Querying the vector store with a text query (need embedding model)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarclays wants to trade in India finance sector with exposure of INR 1 billion. Please suggest RWA optimization strategy adhering to Basel 3 regulations.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m(query_text)  \u001b[38;5;66;03m# This should be a function that converts text to vector\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Search for the nearest neighbors (assumes vector store is properly set up)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m results \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39mquery(query_embedding, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\pydantic\\main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AzureOpenAIEmbedding' object has no attribute 'get_embedding'"
     ]
    }
   ],
   "source": [
    "# Example: Querying the vector store with a text query (need embedding model)\n",
    "query_text = \"Barclays wants to trade in India finance sector with exposure of INR 1 billion. Please suggest RWA optimization strategy adhering to Basel 3 regulations.\"\n",
    "query_embedding = embed_model.get_embedding(query_text)  # This should be a function that converts text to vector\n",
    "\n",
    "# Search for the nearest neighbors (assumes vector store is properly set up)\n",
    "results = vector_store.query(query_embedding, top_k=5)\n",
    "\n",
    "# Print out results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['parent_id']}, Title: {result['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ae852a7-a7f5-421e-99c4-669842b36934",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarclays wants to trade in India finance sector with exposure of INR 1 billion. Please suggest RWA optimization strategy adhering to Basel 3 regulations.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_query_engine(llm, similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print the response\u001b[39;00m\n\u001b[0;32m     10\u001b[0m display_response(response)\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:178\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    176\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    177\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 178\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    180\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[0;32m    181\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[0;32m    183\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:133\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[1;32m--> 133\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle)\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\base\\base_retriever.py:245\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    242\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[0;32m    243\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[0;32m    244\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[1;32m--> 245\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001b[0;32m    247\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[0;32m    248\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[0;32m    249\u001b[0m         )\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:103\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle\u001b[38;5;241m.\u001b[39membedding_strs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     98\u001b[0m         query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     99\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39mget_agg_embedding_from_queries(\n\u001b[0;32m    100\u001b[0m                 query_bundle\u001b[38;5;241m.\u001b[39membedding_strs\n\u001b[0;32m    101\u001b[0m             )\n\u001b[0;32m    102\u001b[0m         )\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_nodes_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:180\u001b[0m, in \u001b[0;36mVectorIndexRetriever._get_nodes_with_embeddings\u001b[1;34m(self, query_bundle_with_embeddings)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_nodes_with_embeddings\u001b[39m(\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m, query_bundle_with_embeddings: QueryBundle\n\u001b[0;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    179\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_vector_store_query(query_bundle_with_embeddings)\n\u001b[1;32m--> 180\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mquery(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_node_list_from_query_result(query_result)\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\vector_stores\\azureaisearch\\base.py:1179\u001b[0m, in \u001b[0;36mAzureAISearchVectorStore.query\u001b[1;34m(self, query, **kwargs)\u001b[0m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m VectorStoreQueryMode\u001b[38;5;241m.\u001b[39mSEMANTIC_HYBRID:\n\u001b[0;32m   1176\u001b[0m     azure_query_result_search \u001b[38;5;241m=\u001b[39m AzureQueryResultSearchSemanticHybrid(\n\u001b[0;32m   1177\u001b[0m         query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_mapping, odata_filter, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_client\n\u001b[0;32m   1178\u001b[0m     )\n\u001b[1;32m-> 1179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mazure_query_result_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\vector_stores\\azureaisearch\\base.py:1474\u001b[0m, in \u001b[0;36mAzureQueryResultSearchBase.search\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1472\u001b[0m search_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_search_query()\n\u001b[0;32m   1473\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_query_vector()\n\u001b[1;32m-> 1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_query_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\vector_stores\\azureaisearch\\base.py:1381\u001b[0m, in \u001b[0;36mAzureQueryResultSearchBase._create_query_result\u001b[1;34m(self, search_query, vectors)\u001b[0m\n\u001b[0;32m   1379\u001b[0m node_id \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_mapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m   1380\u001b[0m metadata_str \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_mapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m-> 1381\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_str\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m metadata_str \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   1382\u001b[0m score \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@search.score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1383\u001b[0m chunk \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_mapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "# Query execution\n",
    "query = \"Barclays wants to trade in India finance sector with exposure of INR 1 billion. Please suggest RWA optimization strategy adhering to Basel 3 regulations.\"\n",
    "query_engine = index.as_query_engine(llm, similarity_top_k=3)\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Print the response\n",
    "display_response(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print what the LLM sees\n",
    "for node in response.source_nodes:\n",
    "    print(node.get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6aeb7325-a52b-49b5-aff9-4d55b491e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adeshpande\\.conda\\envs\\venv\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "688b75b6-f067-4bc6-a029-97042a5c7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "# Set up the Azure OpenAI embedding configuration\n",
    "class AzureOpenAIEmbedding:\n",
    "    def __init__(self, model_name, deployment_name, api_key, azure_endpoint, api_version=\"2024-02-01\"):\n",
    "        self.model_name = model_name\n",
    "        self.deployment_name = deployment_name\n",
    "        self.api_key = api_key\n",
    "        self.azure_endpoint = azure_endpoint\n",
    "        self.api_version = api_version\n",
    "        self.headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': f'Bearer {self.api_key}'\n",
    "        }\n",
    "        \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        # Endpoint to get embeddings\n",
    "        url = f\"{self.azure_endpoint}/openai/deployments/{self.deployment_name}/embeddings?api-version={self.api_version}\"\n",
    "        \n",
    "        # Payload for the request\n",
    "        payload = {\n",
    "            \"input\": [text],\n",
    "            \"model\": self.model_name\n",
    "        }\n",
    "\n",
    "        # Make the API call\n",
    "        response = requests.post(url, headers=self.headers, json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Extract the embeddings from the response\n",
    "            data = response.json()\n",
    "            embedding = data['data'][0]['embedding']\n",
    "            return np.array(embedding)\n",
    "        else:\n",
    "            raise Exception(f\"Error in embedding request: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Create a wrapper for embedding retrieval (this turns it into a simple function)\n",
    "def get_embedding_function(text: str) -> np.ndarray:\n",
    "    embed_model = AzureOpenAIEmbedding(\n",
    "        model_name=\"text-embedding-ada-002\",  # Replace with your model\n",
    "        deployment_name=\"your-deployment-name\",  # Use your deployment name\n",
    "        api_key=\"your-azure-api-key\",  # Your Azure API key\n",
    "        azure_endpoint=\"https://<your-resource-name>.openai.azure.com\",  # Your Azure endpoint\n",
    "        api_version=\"2024-02-01\"  # Ensure this is the correct version\n",
    "    )\n",
    "    return embed_model.get_embedding(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87b08b7e-2a26-4727-bc6a-e92002fcb379",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarclays wants to trade in India finance sector with exposure of INR 1 billion. Please suggest RWA optimization strategy adhering to Basel 3 regulations.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Use the Azure OpenAI model to query the index\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_embedding_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(query)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Display the response from the query engine\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\indices\\base.py:376\u001b[0m, in \u001b[0;36mBaseIndex.as_query_engine\u001b[1;34m(self, llm, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretriever_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    371\u001b[0m     RetrieverQueryEngine,\n\u001b[0;32m    372\u001b[0m )\n\u001b[0;32m    374\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_retriever(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    375\u001b[0m llm \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 376\u001b[0m     \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39mllm\n\u001b[0;32m    379\u001b[0m )\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RetrieverQueryEngine\u001b[38;5;241m.\u001b[39mfrom_args(\n\u001b[0;32m    382\u001b[0m     retriever,\n\u001b[0;32m    383\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    385\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\venv\\lib\\site-packages\\llama_index\\core\\llms\\utils.py:102\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM is explicitly disabled. Using MockLLM.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m     llm \u001b[38;5;241m=\u001b[39m MockLLM()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, LLM)\n\u001b[0;32m    104\u001b[0m llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39mcallback_manager\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Example of creating the query engine with the embedding model\n",
    "# Assume you have an existing `index` object for querying\n",
    "# Ensure you have LlamaIndex and the necessary engine set up for querying\n",
    "\n",
    "# Create the query engine and perform the query\n",
    "query = \"Barclays wants to trade in India finance sector with exposure of INR 1 billion. Please suggest RWA optimization strategy adhering to Basel 3 regulations.\"\n",
    "\n",
    "# Use the Azure OpenAI model to query the index\n",
    "query_engine = index.as_query_engine(llm=get_embedding_function, similarity_top_k=3)\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Display the response from the query engine\n",
    "display_response(response)\n",
    "\n",
    "# Print the full response, including what the LLM sees (the source nodes)\n",
    "for node in response.source_nodes:\n",
    "    print(node.get_content(metadata_mode=MetadataMode.LLM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6199a0-2448-4330-92cd-2e74e4518ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
